{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pronouns = [\n",
    "    \"i\", \"I\", \"I\",\n",
    "    \"me\", \"ME\", \"Me\",\n",
    "    \"my\", \"MY\", \"My\",\n",
    "    \"mine\", \"MINE\", \"Mine\",\n",
    "    \"myself\", \"MYSELF\", \"Myself\",\n",
    "    \"you\", \"YOU\", \"You\",\n",
    "    \"your\", \"YOUR\", \"Your\",\n",
    "    \"yours\", \"YOURS\", \"Yours\",\n",
    "    \"yourself\", \"YOURSELF\", \"Yourself\",\n",
    "    \"yourselves\", \"YOURSELVES\", \"Yourselves\",\n",
    "    \"he\", \"HE\", \"He\",\n",
    "    \"him\", \"HIM\", \"Him\",\n",
    "    \"his\", \"HIS\", \"His\",\n",
    "    \"himself\", \"HIMSELF\", \"Himself\",\n",
    "    \"she\", \"SHE\", \"She\",\n",
    "    \"her\", \"HER\", \"Her\",\n",
    "    \"hers\", \"HERS\", \"Hers\",\n",
    "    \"herself\", \"HERSELF\", \"Herself\",\n",
    "    \"it\", \"IT\", \"It\",\n",
    "    \"its\", \"ITS\", \"Its\",\n",
    "    \"itself\", \"ITSELF\", \"Itself\",\n",
    "    \"we\", \"WE\", \"We\",\n",
    "    \"us\", \"US\", \"Us\",\n",
    "    \"our\", \"OUR\", \"Our\",\n",
    "    \"ours\", \"OURS\", \"Ours\",\n",
    "    \"ourselves\", \"OURSELVES\", \"Ourselves\",\n",
    "    \"they\", \"THEY\", \"They\",\n",
    "    \"them\", \"THEM\", \"Them\",\n",
    "    \"their\", \"THEIR\", \"Their\",\n",
    "    \"theirs\", \"THEIRS\", \"Theirs\",\n",
    "    \"themselves\", \"THEMSELVES\", \"Themselves\",\n",
    "    \"this\", \"THIS\", \"This\",\n",
    "    \"that\", \"THAT\", \"That\",\n",
    "    \"these\", \"THESE\", \"These\",\n",
    "    \"those\", \"THOSE\", \"Those\",\n",
    "    \"who\", \"WHO\", \"Who\",\n",
    "    \"whom\", \"WHOM\", \"Whom\",\n",
    "    \"whose\", \"WHOSE\", \"Whose\",\n",
    "    \"which\", \"WHICH\", \"Which\",\n",
    "    \"what\", \"WHAT\", \"What\",\n",
    "    \"everyone\", \"EVERYONE\", \"Everyone\",\n",
    "    \"someone\", \"SOMEONE\", \"Someone\",\n",
    "    \"anyone\", \"ANYONE\", \"Anyone\",\n",
    "    \"no one\", \"NO ONE\", \"No one\",\n",
    "    \"each\", \"EACH\", \"Each\",\n",
    "    \"either\", \"EITHER\", \"Either\",\n",
    "    \"neither\", \"NEITHER\", \"Neither\",\n",
    "    \"all\", \"ALL\", \"All\",\n",
    "    \"some\", \"SOME\", \"Some\",\n",
    "    \"any\", \"ANY\", \"Any\",\n",
    "    \"none\", \"NONE\", \"None\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = [\n",
    "    \"about\", \"ABOUT\", \"About\",\n",
    "    \"above\", \"ABOVE\", \"Above\",\n",
    "    \"across\", \"ACROSS\", \"Across\",\n",
    "    \"after\", \"AFTER\", \"After\",\n",
    "    \"against\", \"AGAINST\", \"Against\",\n",
    "    \"along\", \"ALONG\", \"Along\",\n",
    "    \"among\", \"AMONG\", \"Among\",\n",
    "    \"around\", \"AROUND\", \"Around\",\n",
    "    \"at\", \"AT\", \"At\",\n",
    "    \"before\", \"BEFORE\", \"Before\",\n",
    "    \"behind\", \"BEHIND\", \"Behind\",\n",
    "    \"below\", \"BELOW\", \"Below\",\n",
    "    \"beneath\", \"BENEATH\", \"Beneath\",\n",
    "    \"beside\", \"BESIDE\", \"Beside\",\n",
    "    \"between\", \"BETWEEN\", \"Between\",\n",
    "    \"beyond\", \"BEYOND\", \"Beyond\",\n",
    "    \"but\", \"BUT\", \"But\",\n",
    "    \"by\", \"BY\", \"By\",\n",
    "    \"concerning\", \"CONCERNING\", \"Concerning\",\n",
    "    \"despite\", \"DESPITE\", \"Despite\",\n",
    "    \"down\", \"DOWN\", \"Down\",\n",
    "    \"during\", \"DURING\", \"During\",\n",
    "    \"except\", \"EXCEPT\", \"Except\",\n",
    "    \"for\", \"FOR\", \"For\",\n",
    "    \"from\", \"FROM\", \"From\",\n",
    "    \"in\", \"IN\", \"In\",\n",
    "    \"inside\", \"INSIDE\", \"Inside\",\n",
    "    \"into\", \"INTO\", \"Into\",\n",
    "    \"like\", \"LIKE\", \"Like\",\n",
    "    \"near\", \"NEAR\", \"Near\",\n",
    "    \"of\", \"OF\", \"Of\",\n",
    "    \"off\", \"OFF\", \"Off\",\n",
    "    \"on\", \"ON\", \"On\",\n",
    "    \"onto\", \"ONTO\", \"Onto\",\n",
    "    \"out\", \"OUT\", \"Out\",\n",
    "    \"outside\", \"OUTSIDE\", \"Outside\",\n",
    "    \"over\", \"OVER\", \"Over\",\n",
    "    \"past\", \"PAST\", \"Past\",\n",
    "    \"regarding\", \"REGARDING\", \"Regarding\",\n",
    "    \"since\", \"SINCE\", \"Since\",\n",
    "    \"through\", \"THROUGH\", \"Through\",\n",
    "    \"throughout\", \"THROUGHOUT\", \"Throughout\",\n",
    "    \"to\", \"TO\", \"To\",\n",
    "    \"toward\", \"TOWARD\", \"Toward\",\n",
    "    \"under\", \"UNDER\", \"Under\",\n",
    "    \"underneath\", \"UNDERNEATH\", \"Underneath\",\n",
    "    \"until\", \"UNTIL\", \"Until\",\n",
    "    \"up\", \"UP\", \"Up\",\n",
    "    \"upon\", \"UPON\", \"Upon\",\n",
    "    \"with\", \"WITH\", \"With\",\n",
    "    \"within\", \"WITHIN\", \"Within\",\n",
    "    \"without\", \"WITHOUT\", \"Without\"\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conjunctions = [\n",
    "    \"and\", \"AND\", \"And\",\n",
    "    \"but\", \"BUT\", \"But\",\n",
    "    \"or\", \"OR\", \"Or\",\n",
    "    \"nor\", \"NOR\", \"Nor\",\n",
    "    \"for\", \"FOR\", \"For\",\n",
    "    \"yet\", \"YET\", \"Yet\",\n",
    "    \"so\", \"SO\", \"So\",\n",
    "    \"although\", \"ALTHOUGH\", \"Although\",\n",
    "    \"because\", \"BECAUSE\", \"Because\",\n",
    "    \"since\", \"SINCE\", \"Since\",\n",
    "    \"unless\", \"UNLESS\", \"Unless\",\n",
    "    \"while\", \"WHILE\", \"While\",\n",
    "    \"though\", \"THOUGH\", \"Though\",\n",
    "    \"if\", \"IF\", \"If\",\n",
    "    \"when\", \"WHEN\", \"When\",\n",
    "    \"where\", \"WHERE\", \"Where\",\n",
    "    \"whether\", \"WHETHER\", \"Whether\",\n",
    "    \"after\", \"AFTER\", \"After\",\n",
    "    \"before\", \"BEFORE\", \"Before\",\n",
    "    \"once\", \"ONCE\", \"Once\",\n",
    "    \"till\", \"TILL\", \"Till\",\n",
    "    \"until\", \"UNTIL\", \"Until\"\n",
    "\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verbs = [\n",
    "    \"am\", \"AM\", \"Am\",\n",
    "    \"is\", \"IS\", \"Is\",\n",
    "    \"are\", \"ARE\", \"Are\",\n",
    "    \"was\", \"WAS\", \"Was\",\n",
    "    \"were\", \"WERE\", \"Were\",\n",
    "    \"be\", \"BE\", \"Be\",\n",
    "    \"been\", \"BEEN\", \"Been\",\n",
    "    \"being\", \"BEING\", \"Being\",\n",
    "    \"have\", \"HAVE\", \"Have\",\n",
    "    \"has\", \"HAS\", \"Has\",\n",
    "    \"had\", \"HAD\", \"Had\",\n",
    "    \"having\", \"HAVING\", \"Having\",\n",
    "    \"do\", \"DO\", \"Do\",\n",
    "    \"does\", \"DOES\", \"Does\",\n",
    "    \"did\", \"DID\", \"Did\",\n",
    "    \"doing\", \"DOING\", \"Doing\",\n",
    "    \"can\", \"CAN\", \"Can\",\n",
    "    \"could\", \"COULD\", \"Could\",\n",
    "    \"may\", \"MAY\", \"May\",\n",
    "    \"might\", \"MIGHT\", \"Might\",\n",
    "    \"shall\", \"SHALL\", \"Shall\",\n",
    "    \"should\", \"SHOULD\", \"Should\",\n",
    "    \"will\", \"WILL\", \"Will\",\n",
    "    \"would\", \"WOULD\", \"Would\",\n",
    "    \"must\", \"MUST\", \"Must\",\n",
    "    \"ought\", \"OUGHT\", \"Ought\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [\n",
    "    \"a\", \"A\", \"a\",\n",
    "    \"an\", \"AN\", \"An\",\n",
    "    \"the\", \"THE\", \"The\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"In recent years, several different approaches have been proposed for reinforcement learning with\n",
    "neural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n",
    "“vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n",
    "[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\n",
    "large models and parallel implementations), data efficient, and robust (i.e., successful on a variety\n",
    "of problems without hyperparameter tuning). Q-learning (with function approximation) fails on\n",
    "many simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\n",
    "effiency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\n",
    "and is not compatible with architectures that include noise (such as dropout) or parameter sharing\n",
    "(between the policy and value function, or with auxiliary tasks).\n",
    "This paper seeks to improve the current state of affairs by introducing an algorithm that attains\n",
    "the data efficiency and reliable performance of TRPO, while using only first-order optimization.\n",
    "We propose a novel objective with clipped probability ratios, which forms a pessimistic estimate\n",
    "(i.e., lower bound) of the performance of the policy. To optimize policies, we alternate between\n",
    "sampling data from the policy and performing several epochs of optimization on the sampled data.\n",
    "Our experiments compare the performance of various different versions of the surrogate objective, and find that the version with the clipped probability ratios performs best. We also compare\n",
    "PPO to several previous algorithms from the literature. On continuous control tasks, it performs\n",
    "better than the algorithms we compare against. On Atari, it performs significantly better (in terms\n",
    "of sample complexity) than A2C and similarly to ACER though it is much simpler.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_ls = text.split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"\"\n",
    "full_s = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 0\n",
    "for i in s_ls:\n",
    "    length += 1\n",
    "    if i in pronouns:\n",
    "        continue\n",
    "    elif i in prepositions:\n",
    "        continue\n",
    "    elif i in conjunctions:\n",
    "        continue\n",
    "    elif i in auxiliary_verbs:\n",
    "        continue\n",
    "    elif i in articles:\n",
    "        continue\n",
    "    else:\n",
    "        s += i + \" \"\n",
    "        if i[-1] == \".\":\n",
    "            full_s.append(s)\n",
    "            s = \"\"\n",
    "        if length == len(s_ls):\n",
    "            if i[-1] != \".\":\n",
    "                full_s.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recent years, several different approaches proposed reinforcement learning with\n",
      "neural network function approximators. \n",
      "\n",
      "\n",
      "leading contenders deep Q-learning [Mni+15],\n",
      "“vanilla” policy gradient methods [Mni+16], trust region / natural policy gradient methods\n",
      "[Sch+15b]. \n",
      "\n",
      "\n",
      "However, there room improvement developing method scalable (to\n",
      "large models parallel implementations), data efficient, robust (i.e., successful variety\n",
      "of problems hyperparameter tuning). \n",
      "\n",
      "\n",
      "Q-learning (with function approximation) fails on\n",
      "many simple problems1 poorly understood, vanilla policy gradient methods poor data\n",
      "effiency robustness; trust region policy optimization (TRPO) relatively complicated,\n",
      "and not compatible architectures include noise (such as dropout) parameter sharing\n",
      "(between policy value function, auxiliary tasks).\n",
      "This paper seeks improve current state affairs introducing algorithm attains\n",
      "the data efficiency reliable performance TRPO, using only first-order optimization.\n",
      "We propose novel objective clipped probability ratios, forms pessimistic estimate\n",
      "(i.e., lower bound) performance policy. \n",
      "\n",
      "\n",
      "optimize policies, alternate between\n",
      "sampling data policy performing several epochs optimization sampled data.\n",
      "Our experiments compare performance various different versions surrogate objective, find version clipped probability ratios performs best. \n",
      "\n",
      "\n",
      "also compare\n",
      "PPO several previous algorithms literature. \n",
      "\n",
      "\n",
      "continuous control tasks, performs\n",
      "better than algorithms compare against. \n",
      "\n",
      "\n",
      "Atari, performs significantly better (in terms\n",
      "of sample complexity) than A2C similarly ACER much simpler. \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k in full_s:\n",
    "    print(k)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = \"\"\"\n",
    "In recent years, several different approaches have been proposed for reinforcement learning with\n",
    "neural network function approximators. The leading contenders are deep Q-learning [Mni+15],\n",
    "“vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods\n",
    "[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\n",
    "large models and parallel implementations), data efficient, and robust (i.e., successful on a variety\n",
    "of problems without hyperparameter tuning). Q-learning (with function approximation) fails on\n",
    "many simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\n",
    "effiency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\n",
    "and is not compatible with architectures that include noise (such as dropout) or parameter sharing\n",
    "(between the policy and value function, or with auxiliary tasks).\n",
    "This\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn recent years, several different approaches have been proposed for reinforcement learning with\\nneural network function approximators. The leading contenders are deep Q-learning [Mni+15],\\n“vanilla” policy gradient methods [Mni+16], and trust region / natural policy gradient methods\\n[Sch+15b]. However, there is room for improvement in developing a method that is scalable (to\\nlarge models and parallel implementations), data efficient, and robust (i.e., successful on a variety\\nof problems without hyperparameter tuning). Q-learning (with function approximation) fails on\\nmany simple problems1 and is poorly understood, vanilla policy gradient methods have poor data\\neffiency and robustness; and trust region policy optimization (TRPO) is relatively complicated,\\nand is not compatible with architectures that include noise (such as dropout) or parameter sharing\\n(between the policy and value function, or with auxiliary tasks).\\nThis\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
